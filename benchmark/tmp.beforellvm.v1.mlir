
// -----// IR Dump Before ConvertTritonAMDGPUToLLVM (convert-triton-amdgpu-to-llvm) ('builtin.module' operation) //----- //
#blocked = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [64, 1], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#linear = #ttg.linear<{register = [[1, 0], [2, 0], [4, 0]], lane = [[8, 0], [16, 0], [32, 0], [0, 4], [0, 8], [0, 16]], warp = [[0, 1], [0, 2]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2], [0, 4], [32, 0]], lane = [[0, 8], [0, 16], [0, 32], [4, 0], [8, 0], [16, 0]], warp = [[1, 0], [2, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[1, 0], [2, 0], [4, 0], [0, 16], [32, 0], [64, 0], [128, 0], [256, 0]], lane = [[0, 1], [0, 2], [0, 4], [0, 8], [8, 0], [16, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear3 = #ttg.linear<{register = [[0, 1], [0, 2], [0, 4], [16, 0], [0, 32], [0, 64], [0, 128], [0, 256]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [0, 8], [0, 16]], warp = [[0, 0], [0, 0]], block = []}>
#loc = loc(unknown)
#loc1 = loc("Q_nope")
#loc2 = loc("Q_pe")
#loc3 = loc("Kv_c_cache")
#loc4 = loc("K_pe_cache")
#loc5 = loc("Req_to_tokens")
#loc6 = loc("B_seq_len")
#loc7 = loc("O")
#loc8 = loc("sm_scale")
#loc9 = loc("stride_q_nope_bs")
#loc10 = loc("stride_q_nope_h")
#loc11 = loc("stride_q_pe_bs")
#loc12 = loc("stride_q_pe_h")
#loc13 = loc("stride_kv_c_bs")
#loc14 = loc("stride_k_pe_bs")
#loc15 = loc("stride_req_to_tokens_bs")
#loc16 = loc("stride_o_b")
#loc17 = loc("stride_o_h")
#loc18 = loc("stride_o_s")
#loc19 = loc("e_max")
#loc20 = loc("start_n")
#loc21 = loc("e_sum")
#loc22 = loc("acc")
#mma = #ttg.amd_mfma<{version = 4, warpsPerCTA = [4, 1], instrShape = [16, 16, 32], isTransposed = true}>
#shared = #ttg.padded_shared<[512:+16] {order = [1, 0], shape = [64, 512]}>
#shared1 = #ttg.padded_shared<[512:+16] {offset = [[0, 1], [0, 2], [0, 4], [0, 8], [0, 16], [0, 32], [4, 0], [8, 0], [16, 0], [1, 0], [2, 0], [32, 0]], block = []}>
#shared2 = #ttg.swizzled_shared<{vec = 8, perPhase = 1, maxPhase = 16, order = [0, 1]}>
#shared3 = #ttg.padded_shared<[512:+16] {offset = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [32, 0], [0, 4], [0, 8], [0, 16], [0, 1], [0, 2]], block = []}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.shared = 112928 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32, "ttg.total-num-warps" = 4 : i32} {
  tt.func public @_mla_attn_kernel_gluon(%Q_nope: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("Q_nope"), %Q_pe: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("Q_pe"), %Kv_c_cache: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("Kv_c_cache"), %K_pe_cache: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("K_pe_cache"), %Req_to_tokens: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("Req_to_tokens"), %B_seq_len: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("B_seq_len"), %O: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("O"), %sm_scale: f32 loc("sm_scale"), %stride_q_nope_bs: i32 {tt.divisibility = 16 : i32} loc("stride_q_nope_bs"), %stride_q_nope_h: i32 {tt.divisibility = 16 : i32} loc("stride_q_nope_h"), %stride_q_pe_bs: i32 {tt.divisibility = 16 : i32} loc("stride_q_pe_bs"), %stride_q_pe_h: i32 {tt.divisibility = 16 : i32} loc("stride_q_pe_h"), %stride_kv_c_bs: i32 {tt.divisibility = 16 : i32} loc("stride_kv_c_bs"), %stride_k_pe_bs: i32 {tt.divisibility = 16 : i32} loc("stride_k_pe_bs"), %stride_req_to_tokens_bs: i32 loc("stride_req_to_tokens_bs"), %stride_o_b: i32 {tt.divisibility = 16 : i32} loc("stride_o_b"), %stride_o_h: i32 loc("stride_o_h"), %stride_o_s: i32 loc("stride_o_s")) attributes {noinline = false} {
    %cst = arith.constant dense<0xFF800000> : tensor<64x32xf32, #mma> loc(#loc)
    %cst_0 = arith.constant dense<64> : tensor<32xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc)
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x32xf32, #mma> loc(#loc)
    %cst_2 = arith.constant dense<64> : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc)
    %cst_3 = arith.constant dense<512> : tensor<64xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<64x512xf32, #mma> loc(#loc)
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %cst_6 = arith.constant dense<0xFF800000> : tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %0 = tt.get_program_id y : i32 loc(#loc)
    %1 = tt.get_program_id x : i32 loc(#loc)
    %2 = tt.get_program_id z : i32 loc(#loc)
    %3 = tt.addptr %B_seq_len, %0 : !tt.ptr<i32>, i32 loc(#loc)
    %4 = tt.load %3 : !tt.ptr<i32> loc(#loc)
    %5 = ttg.local_alloc {allocation.offset = 0 : i32} : () -> !ttg.memdesc<64x512xbf16, #shared, #smem, mutable> loc(#loc)
    %6 = ttg.local_alloc {allocation.offset = 100320 : i32} : () -> !ttg.memdesc<64x64xbf16, #shared1, #smem, mutable> loc(#loc)
    %7 = ttg.local_alloc {allocation.offset = 67552 : i32} : () -> !ttg.memdesc<512x32xbf16, #shared2, #smem, mutable> loc(#loc)
    %8 = ttg.local_alloc {allocation.offset = 108736 : i32} : () -> !ttg.memdesc<64x32xbf16, #shared3, #smem, mutable> loc(#loc)
    %9 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc)
    %10 = arith.muli %1, %c64_i32 : i32 loc(#loc)
    %11 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc)
    %12 = tt.splat %10 : i32 -> tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc)
    %13 = arith.addi %12, %11 : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc)
    %14 = arith.muli %0, %stride_q_nope_bs : i32 loc(#loc)
    %15 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<64x1xi32, #blocked1> loc(#loc)
    %16 = tt.splat %stride_q_nope_h : i32 -> tensor<64x1xi32, #blocked1> loc(#loc)
    %17 = arith.muli %15, %16 : tensor<64x1xi32, #blocked1> loc(#loc)
    %18 = tt.splat %14 : i32 -> tensor<64x1xi32, #blocked1> loc(#loc)
    %19 = arith.addi %18, %17 : tensor<64x1xi32, #blocked1> loc(#loc)
    %20 = tt.expand_dims %9 {axis = 0 : i32} : tensor<512xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x512xi32, #blocked1> loc(#loc)
    %21 = tt.broadcast %19 : tensor<64x1xi32, #blocked1> -> tensor<64x512xi32, #blocked1> loc(#loc)
    %22 = tt.broadcast %20 : tensor<1x512xi32, #blocked1> -> tensor<64x512xi32, #blocked1> loc(#loc)
    %23 = arith.addi %21, %22 : tensor<64x512xi32, #blocked1> loc(#loc)
    %24 = amdg.buffer_load_to_local %Q_nope[%23] into %5 : <bf16>[tensor<64x512xi32, #blocked1>]  -> <64x512xbf16, #shared, #smem, mutable> loc(#loc)
    %25 = ttg.async_commit_group loc(#loc)
    %26 = amdg.async_wait {num_inst = 0 : i32} loc(#loc)
    %27 = ttg.local_load %5 : !ttg.memdesc<64x512xbf16, #shared, #smem, mutable> -> tensor<64x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc)
    %28 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear1}>> loc(#loc)
    %29 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #linear1}>> loc(#loc)
    %30 = tt.splat %10 : i32 -> tensor<64xi32, #ttg.slice<{dim = 1, parent = #linear1}>> loc(#loc)
    %31 = arith.addi %30, %29 : tensor<64xi32, #ttg.slice<{dim = 1, parent = #linear1}>> loc(#loc)
    %32 = arith.muli %0, %stride_q_pe_bs : i32 loc(#loc)
    %33 = tt.expand_dims %31 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<64x1xi32, #linear1> loc(#loc)
    %34 = tt.splat %stride_q_pe_h : i32 -> tensor<64x1xi32, #linear1> loc(#loc)
    %35 = arith.muli %33, %34 : tensor<64x1xi32, #linear1> loc(#loc)
    %36 = tt.splat %32 : i32 -> tensor<64x1xi32, #linear1> loc(#loc)
    %37 = arith.addi %36, %35 : tensor<64x1xi32, #linear1> loc(#loc)
    %38 = tt.expand_dims %28 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear1}>> -> tensor<1x64xi32, #linear1> loc(#loc)
    %39 = tt.broadcast %37 : tensor<64x1xi32, #linear1> -> tensor<64x64xi32, #linear1> loc(#loc)
    %40 = tt.broadcast %38 : tensor<1x64xi32, #linear1> -> tensor<64x64xi32, #linear1> loc(#loc)
    %41 = arith.addi %39, %40 : tensor<64x64xi32, #linear1> loc(#loc)
    %42 = amdg.buffer_load_to_local %Q_pe[%41] into %6 : <bf16>[tensor<64x64xi32, #linear1>]  -> <64x64xbf16, #shared1, #smem, mutable> loc(#loc)
    %43 = ttg.async_commit_group loc(#loc)
    %44 = amdg.async_wait {num_inst = 0 : i32} loc(#loc)
    %45 = ttg.local_load %6 : !ttg.memdesc<64x64xbf16, #shared1, #smem, mutable> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc)
    %46 = arith.muli %4, %2 : i32 loc(#loc)
    %47 = arith.addi %46, %4 : i32 loc(#loc)
    %48 = arith.minsi %47, %4 : i32 loc(#loc)
    cf.br ^bb1(%46, %cst_6, %cst_5, %cst_4 : i32, tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<64x512xf32, #mma>) loc(#loc24)
  ^bb1(%start_n: i32 loc("start_n"), %e_max: tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc("e_max"), %e_sum: tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc("e_sum"), %acc: tensor<64x512xf32, #mma> loc("acc")):  // 2 preds: ^bb0, ^bb2
    %acc_7 = arith.cmpi slt, %start_n, %48 : i32 loc(#loc24)
    cf.cond_br %acc_7, ^bb2, ^bb3 loc(#loc24)
  ^bb2:  // pred: ^bb1
    %49 = arith.muli %stride_req_to_tokens_bs, %0 : i32 loc(#loc)
    %50 = tt.addptr %Req_to_tokens, %49 : !tt.ptr<i32>, i32 loc(#loc)
    %51 = arith.divsi %start_n, %c64_i32 : i32 loc(#loc)
    %52 = tt.addptr %50, %51 : !tt.ptr<i32>, i32 loc(#loc)
    %53 = tt.load %52 : !tt.ptr<i32> loc(#loc)
    %54 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc)
    %55 = tt.splat %start_n : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc)
    %56 = arith.addi %55, %54 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc)
    %57 = arith.muli %53, %c64_i32 : i32 loc(#loc)
    %58 = arith.remsi %56, %cst_2 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc)
    %59 = tt.splat %57 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc)
    %60 = arith.addi %59, %58 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc)
    %61 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc)
    %62 = tt.expand_dims %60 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x32xi32, #blocked> loc(#loc)
    %63 = tt.splat %stride_kv_c_bs : i32 -> tensor<1x32xi32, #blocked> loc(#loc)
    %64 = arith.muli %62, %63 : tensor<1x32xi32, #blocked> loc(#loc)
    %65 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<512x1xi32, #blocked> loc(#loc)
    %66 = tt.broadcast %64 : tensor<1x32xi32, #blocked> -> tensor<512x32xi32, #blocked> loc(#loc)
    %67 = tt.broadcast %65 : tensor<512x1xi32, #blocked> -> tensor<512x32xi32, #blocked> loc(#loc)
    %68 = arith.addi %66, %67 : tensor<512x32xi32, #blocked> loc(#loc)
    %69 = tt.expand_dims %56 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x32xi32, #blocked> loc(#loc)
    %70 = tt.splat %48 : i32 -> tensor<1x32xi32, #blocked> loc(#loc)
    %71 = arith.cmpi slt, %69, %70 : tensor<1x32xi32, #blocked> loc(#loc)
    %72 = tt.broadcast %71 : tensor<1x32xi1, #blocked> -> tensor<512x32xi1, #blocked> loc(#loc)
    %73 = amdg.buffer_load_to_local %Kv_c_cache[%68] mask = %72 into %7 : <bf16>[tensor<512x32xi32, #blocked>]  -> <512x32xbf16, #shared2, #smem, mutable> loc(#loc)
    %74 = ttg.async_commit_group loc(#loc)
    %75 = amdg.async_wait {num_inst = 0 : i32} loc(#loc)
    %76 = ttg.local_load %7 : !ttg.memdesc<512x32xbf16, #shared2, #smem, mutable> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc)
    %77 = tt.dot %27, %76, %cst_1 : tensor<64x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<64x32xf32, #mma> loc(#loc)
    %78 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc)
    %79 = tt.splat %start_n : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc)
    %80 = arith.addi %79, %78 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc)
    %81 = arith.remsi %80, %cst_0 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc)
    %82 = tt.splat %57 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc)
    %83 = arith.addi %82, %81 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc)
    %84 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc)
    %85 = tt.expand_dims %83 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #linear}>> -> tensor<1x32xi32, #linear> loc(#loc)
    %86 = tt.splat %stride_k_pe_bs : i32 -> tensor<1x32xi32, #linear> loc(#loc)
    %87 = arith.muli %85, %86 : tensor<1x32xi32, #linear> loc(#loc)
    %88 = tt.expand_dims %84 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #linear}>> -> tensor<64x1xi32, #linear> loc(#loc)
    %89 = tt.broadcast %87 : tensor<1x32xi32, #linear> -> tensor<64x32xi32, #linear> loc(#loc)
    %90 = tt.broadcast %88 : tensor<64x1xi32, #linear> -> tensor<64x32xi32, #linear> loc(#loc)
    %91 = arith.addi %89, %90 : tensor<64x32xi32, #linear> loc(#loc)
    %92 = tt.expand_dims %80 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #linear}>> -> tensor<1x32xi32, #linear> loc(#loc)
    %93 = tt.splat %48 : i32 -> tensor<1x32xi32, #linear> loc(#loc)
    %94 = arith.cmpi slt, %92, %93 : tensor<1x32xi32, #linear> loc(#loc)
    %95 = tt.broadcast %94 : tensor<1x32xi1, #linear> -> tensor<64x32xi1, #linear> loc(#loc)
    %96 = amdg.buffer_load_to_local %K_pe_cache[%91] mask = %95 into %8 : <bf16>[tensor<64x32xi32, #linear>]  -> <64x32xbf16, #shared3, #smem, mutable> loc(#loc)
    %97 = ttg.async_commit_group loc(#loc)
    %98 = amdg.async_wait {num_inst = 0 : i32} loc(#loc)
    %99 = ttg.local_load %8 : !ttg.memdesc<64x32xbf16, #shared3, #smem, mutable> -> tensor<64x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc)
    %100 = tt.dot %45, %99, %77 : tensor<64x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<64x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<64x32xf32, #mma> loc(#loc)
    %101 = tt.splat %sm_scale : f32 -> tensor<64x32xf32, #mma> loc(#loc)
    %102 = arith.mulf %100, %101 : tensor<64x32xf32, #mma> loc(#loc)
    %103 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc)
    %104 = tt.splat %start_n : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc)
    %105 = arith.addi %104, %103 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc)
    %106 = tt.expand_dims %105 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #mma}>> -> tensor<1x32xi32, #mma> loc(#loc)
    %107 = tt.splat %48 : i32 -> tensor<1x32xi32, #mma> loc(#loc)
    %108 = arith.cmpi slt, %106, %107 : tensor<1x32xi32, #mma> loc(#loc)
    %109 = tt.broadcast %108 : tensor<1x32xi1, #mma> -> tensor<64x32xi1, #mma> loc(#loc)
    %110 = arith.select %109, %102, %cst : tensor<64x32xi1, #mma>, tensor<64x32xf32, #mma> loc(#loc)
    %111 = "tt.reduce"(%110) <{axis = 1 : i32}> ({
    ^bb0(%arg18: f32 loc(unknown), %arg19: f32 loc(unknown)):
      %162 = arith.maxnumf %arg18, %arg19 : f32 loc(#loc)
      tt.reduce.return %162 : f32 loc(#loc)
    }) : (tensor<64x32xf32, #mma>) -> tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %112 = arith.maxnumf %111, %e_max : tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %113 = arith.subf %e_max, %112 : tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %114 = math.exp %113 : tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %115 = tt.expand_dims %112 {axis = 1 : i32} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<64x1xf32, #mma> loc(#loc)
    %116 = tt.broadcast %115 : tensor<64x1xf32, #mma> -> tensor<64x32xf32, #mma> loc(#loc)
    %117 = arith.subf %110, %116 : tensor<64x32xf32, #mma> loc(#loc)
    %118 = math.exp %117 : tensor<64x32xf32, #mma> loc(#loc)
    %119 = arith.mulf %e_sum, %114 : tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %120 = "tt.reduce"(%118) <{axis = 1 : i32}> ({
    ^bb0(%arg18: f32 loc(unknown), %arg19: f32 loc(unknown)):
      %162 = arith.addf %arg18, %arg19 : f32 loc(#loc)
      tt.reduce.return %162 : f32 loc(#loc)
    }) : (tensor<64x32xf32, #mma>) -> tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %121 = arith.addf %119, %120 : tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %122 = arith.truncf %118 : tensor<64x32xf32, #mma> to tensor<64x32xbf16, #mma> loc(#loc)
    %123 = ttg.convert_layout %122 : tensor<64x32xbf16, #mma> -> tensor<64x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc)
    %124 = tt.expand_dims %114 {axis = 1 : i32} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<64x1xf32, #mma> loc(#loc)
    %125 = tt.broadcast %124 : tensor<64x1xf32, #mma> -> tensor<64x512xf32, #mma> loc(#loc)
    %126 = arith.mulf %acc, %125 : tensor<64x512xf32, #mma> loc(#loc)
    %127 = ttg.local_load %7 : !ttg.memdesc<512x32xbf16, #shared2, #smem, mutable> -> tensor<512x32xbf16, #linear2> loc(#loc)
    %128 = tt.trans %127 {order = array<i32: 1, 0>} : tensor<512x32xbf16, #linear2> -> tensor<32x512xbf16, #linear3> loc(#loc)
    %129 = ttg.convert_layout %128 {allocation.offset = 0 : i32} : tensor<32x512xbf16, #linear3> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc)
    %130 = tt.dot %123, %129, %126 : tensor<64x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<32x512xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<64x512xf32, #mma> loc(#loc)
    %acc_8 = arith.addi %start_n, %c32_i32 : i32 loc(#loc24)
    cf.br ^bb1(%acc_8, %112, %121, %130 : i32, tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<64x512xf32, #mma>) loc(#loc24)
  ^bb3:  // pred: ^bb1
    %131 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %132 = tt.splat %10 : i32 -> tensor<64xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %133 = arith.addi %132, %131 : tensor<64xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %134 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc)
    %135 = arith.muli %0, %stride_o_b : i32 loc(#loc)
    %136 = tt.expand_dims %133 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<64x1xi32, #mma> loc(#loc)
    %137 = tt.splat %stride_o_h : i32 -> tensor<64x1xi32, #mma> loc(#loc)
    %138 = arith.muli %136, %137 : tensor<64x1xi32, #mma> loc(#loc)
    %139 = tt.splat %135 : i32 -> tensor<64x1xi32, #mma> loc(#loc)
    %140 = arith.addi %139, %138 : tensor<64x1xi32, #mma> loc(#loc)
    %141 = arith.muli %2, %stride_o_s : i32 loc(#loc)
    %142 = tt.splat %141 : i32 -> tensor<64x1xi32, #mma> loc(#loc)
    %143 = arith.addi %140, %142 : tensor<64x1xi32, #mma> loc(#loc)
    %144 = tt.expand_dims %134 {axis = 0 : i32} : tensor<512xi32, #ttg.slice<{dim = 0, parent = #mma}>> -> tensor<1x512xi32, #mma> loc(#loc)
    %145 = tt.broadcast %143 : tensor<64x1xi32, #mma> -> tensor<64x512xi32, #mma> loc(#loc)
    %146 = tt.broadcast %144 : tensor<1x512xi32, #mma> -> tensor<64x512xi32, #mma> loc(#loc)
    %147 = arith.addi %145, %146 : tensor<64x512xi32, #mma> loc(#loc)
    %148 = tt.expand_dims %e_sum {axis = 1 : i32} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<64x1xf32, #mma> loc(#loc)
    %149 = tt.broadcast %148 : tensor<64x1xf32, #mma> -> tensor<64x512xf32, #mma> loc(#loc)
    %150 = arith.divf %acc, %149 : tensor<64x512xf32, #mma> loc(#loc)
    %151 = arith.truncf %150 : tensor<64x512xf32, #mma> to tensor<64x512xbf16, #mma> loc(#loc)
    amdg.buffer_store %151, %O[%147] : tensor<64x512xbf16, #mma> loc(#loc)
    %152 = tt.splat %stride_o_h : i32 -> tensor<64xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %153 = arith.muli %133, %152 : tensor<64xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %154 = tt.splat %135 : i32 -> tensor<64xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %155 = arith.addi %154, %153 : tensor<64xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %156 = tt.splat %141 : i32 -> tensor<64xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %157 = arith.addi %155, %156 : tensor<64xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %158 = arith.addi %157, %cst_3 : tensor<64xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %159 = math.log %e_sum : tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %160 = arith.addf %e_max, %159 : tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    %161 = arith.truncf %160 : tensor<64xf32, #ttg.slice<{dim = 1, parent = #mma}>> to tensor<64xbf16, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    amdg.buffer_store %161, %O[%158] : tensor<64xbf16, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc23 = loc("e_sum"(#loc19))
#loc24 = loc("acc"(#loc23))
