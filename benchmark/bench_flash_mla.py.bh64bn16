@gluon.jit
def _mla_attn_kernel_gluon(
    Q_nope,
    Q_pe,
    Kv_c_cache,
    K_pe_cache,
    Req_to_tokens,
    B_seq_len,
    O,
    sm_scale,
    stride_q_nope_bs,
    stride_q_nope_h,
    stride_q_pe_bs,
    stride_q_pe_h,
    stride_kv_c_bs,
    stride_k_pe_bs,
    stride_req_to_tokens_bs,
    stride_o_b,
    stride_o_h,
    stride_o_s,
    BLOCK_H: gl.constexpr,
    BLOCK_N: gl.constexpr,
    NUM_KV_SPLITS: gl.constexpr,
    PAGE_SIZE: gl.constexpr,
    HEAD_DIM_CKV: gl.constexpr,
    HEAD_DIM_KPE: gl.constexpr,
):
    cur_batch = gl.program_id(1)
    cur_head_id = gl.program_id(0)
    split_kv_id = gl.program_id(2)

    cur_batch_seq_len = gl.load(B_seq_len + cur_batch)


    # layout for Q
    # 64x512
    blocked_q_nope: gl.constexpr = gl.BlockedLayout(
        size_per_thread=[1, 8],
        threads_per_warp=[1, 64],
        warps_per_cta=[4, 1],
        order=[1, 0],
    )
    shared_q_nope: gl.constexpr = gl.PaddedSharedLayout(
        interval_padding_pairs = [[512,16]],
        offset_bases = [[0, 1], [0, 2], [0, 4], [0, 8], [0, 16], [0, 32], [0, 64], [0, 128], [0, 256], [1,0], [2,0], [4,0], [8,0], [16,0], [32,0]],
        block_bases = [],
        shape = [64, 512]
    )
    # 64x64
    blocked_q_pe: gl.constexpr = gl.DistributedLinearLayout(
        reg_bases=((0,1),(0,2), (0,4), (32, 0)),
        lane_bases=((0, 8), (0, 16), (0, 32), (4, 0), (8, 0), (16, 0)),
        warp_bases=((1, 0), (2, 0)),
        block_bases=[],
        shape=[64, 64],
    )
    shared_q_pe: gl.constexpr = gl.PaddedSharedLayout(
        interval_padding_pairs = [[512,16]],
        offset_bases = [[0, 1], [0, 2], [0, 4], [0, 8], [0, 16], [0, 32], [4,0], [8,0], [16, 0], [1,0], [2,0], [32, 0]],
        block_bases = [],
        shape = [64, 64]
    )
    buf_q_nope = gl.allocate_shared_memory(Q_nope.dtype, shape=[BLOCK_H, HEAD_DIM_CKV], layout=shared_q_nope)
    buf_q_pe = gl.allocate_shared_memory(Q_pe.dtype, shape=[BLOCK_H, HEAD_DIM_KPE], layout=shared_q_pe)

    # layout for KV
    # 512x16
    blocked_kv: gl.constexpr = gl.BlockedLayout(
        size_per_thread=[8, 1],
        threads_per_warp=[64, 1],
        warps_per_cta=[1, 4],
        order=[0, 1],
    )
    # 64x16
    # to align with kv on dim1
    blocked_kpe: gl.constexpr = gl.BlockedLayout(
        size_per_thread=[1, 1],
        threads_per_warp=[64, 1],
        warps_per_cta=[1, 4],
        order=[0, 1],
    )
        # size_per_thread=[4, 1],
        # threads_per_warp=[16, 4],

    shared_kv: gl.constexpr = gl.SwizzledSharedLayout(vec=8, per_phase=1, max_phase=16, order=[0, 1])
    shared_kpe: gl.constexpr = gl.SwizzledSharedLayout(vec=8, per_phase=2, max_phase=8, order=[0, 1])
    #bufs_kv = gl.allocate_shared_memory(Kv_c_cache.dtype, shape=[2, HEAD_DIM_CKV, BLOCK_N], layout=shared_kv)
    #bufs_kpe = gl.allocate_shared_memory(K_pe_cache.dtype, shape=[2, HEAD_DIM_KPE, BLOCK_N], layout=shared_kpe)
    buf_kv = gl.allocate_shared_memory(Kv_c_cache.dtype, shape=[HEAD_DIM_CKV, BLOCK_N], layout=shared_kv)
    buf_kpe = gl.allocate_shared_memory(K_pe_cache.dtype, shape=[HEAD_DIM_KPE, BLOCK_N], layout=shared_kpe)

    # layout for mfma
    mfma_layout: gl.constexpr = gl.amd.AMDMFMALayout(
        version=4,
        instr_shape=[16, 16, 32],
        transposed=True,
        warps_per_cta=[4, 1],
    )
    mfma_layout_a: gl.constexpr = gl.DotOperandLayout(
        operand_index=0, parent=mfma_layout, k_width=8
    )
    mfma_layout_b: gl.constexpr = gl.DotOperandLayout(
        operand_index=1, parent=mfma_layout, k_width=8
    )


    offs_d_ckv = gl.arange(0, HEAD_DIM_CKV, layout=gl.SliceLayout(0, blocked_q_nope))
    cur_head = cur_head_id * BLOCK_H + gl.arange(0, BLOCK_H, layout=gl.SliceLayout(1, blocked_q_nope))
    offs_q_nope = cur_batch * stride_q_nope_bs + cur_head[:, None] * stride_q_nope_h + offs_d_ckv[None, :]
    # q_nope = gl.load(Q_nope + offs_q_nope)
    gl.amd.cdna4.async_copy.buffer_load_to_shared(buf_q_nope, Q_nope, offs_q_nope)
    gl.amd.cdna4.async_copy.commit_group()
    gl.amd.cdna4.async_copy.wait_group(0)
    q_nope = buf_q_nope.load(layout=mfma_layout_a)

    offs_d_kpe = gl.arange(0, HEAD_DIM_KPE, layout=gl.SliceLayout(0, blocked_q_pe))
    cur_head_qpe = cur_head_id * BLOCK_H + gl.arange(0, BLOCK_H, layout=gl.SliceLayout(1, blocked_q_pe))
    offs_q_pe = cur_batch * stride_q_pe_bs + cur_head_qpe[:, None] * stride_q_pe_h + offs_d_kpe[None, :]
    # q_pe = gl.load(Q_pe + offs_q_pe)
    gl.amd.cdna4.async_copy.buffer_load_to_shared(buf_q_pe, Q_pe, offs_q_pe)
    gl.amd.cdna4.async_copy.commit_group()
    gl.amd.cdna4.async_copy.wait_group(0)
    q_pe = buf_q_pe.load(layout=mfma_layout_a)

    e_max = gl.zeros([BLOCK_H], dtype=gl.float32, layout=gl.SliceLayout(1, mfma_layout)) - float("inf")
    e_sum = gl.zeros([BLOCK_H], dtype=gl.float32, layout=gl.SliceLayout(1, mfma_layout))
    acc = gl.zeros([BLOCK_H, HEAD_DIM_CKV], dtype=gl.float32, layout=mfma_layout)

    kv_len_per_split = gl.cdiv(cur_batch_seq_len, NUM_KV_SPLITS)
    split_kv_start = kv_len_per_split * split_kv_id
    split_kv_end = gl.minimum(split_kv_start + kv_len_per_split, cur_batch_seq_len)

    for start_n in range(split_kv_start, split_kv_end, BLOCK_N):
        offs_n = start_n + gl.arange(0, BLOCK_N, layout=gl.SliceLayout(0, blocked_kv))
        kv_page_number = gl.load(
            Req_to_tokens + stride_req_to_tokens_bs * cur_batch + offs_n // PAGE_SIZE,
            mask=offs_n < split_kv_end,
            other=0,
        )
        kv_loc = kv_page_number * PAGE_SIZE + offs_n % PAGE_SIZE
        offs_d_ckv_1 = gl.arange(0, HEAD_DIM_CKV, layout=gl.SliceLayout(1, blocked_kv))
        offs_k_c = kv_loc[None, :] * stride_kv_c_bs + offs_d_ckv_1[:, None]
        # k_c = gl.load(Kv_c_cache + offs_k_c, mask=offs_n[None, :] < split_kv_end, other=0.0)
        gl.amd.cdna4.async_copy.buffer_load_to_shared(buf_kv, Kv_c_cache, offs_k_c, mask=offs_n[None, :] < split_kv_end)
        gl.amd.cdna4.async_copy.commit_group()
        gl.amd.cdna4.async_copy.wait_group(0)
        k_c = buf_kv.load(layout=mfma_layout_b)

        zeros = gl.zeros([BLOCK_H, BLOCK_N], dtype=gl.float32, layout=mfma_layout)
        qk = gl.amd.cdna4.mfma(q_nope, k_c.to(q_nope.dtype), zeros)

        offs_d_kpe_1 = gl.arange(0, HEAD_DIM_KPE, layout=gl.SliceLayout(1, blocked_kpe))
        offs_k_pe = kv_loc[None, :] * stride_k_pe_bs + offs_d_kpe_1[:, None]
        # k_pe = gl.load(K_pe_cache + offs_k_pe, mask=offs_n[None, :] < split_kv_end, other=0.0)
        gl.amd.cdna4.async_copy.buffer_load_to_shared(buf_kpe, K_pe_cache, offs_k_pe, mask=offs_n[None, :] < split_kv_end)
        gl.amd.cdna4.async_copy.commit_group()
        gl.amd.cdna4.async_copy.wait_group(0)
        k_pe = buf_kpe.load(layout=mfma_layout_b)

        qk = gl.amd.cdna4.mfma(q_pe, k_pe.to(q_pe.dtype), qk)
        qk *= sm_scale

        qk = gl.where(offs_n[None, :] < split_kv_end, qk, float("-inf"))


        # cross warp reduction
        n_e_max = gl.maximum(gl.max(qk, 1), e_max)

        re_scale = gl.exp(e_max - n_e_max)
        p = gl.exp(qk - n_e_max[:, None])
        # move around p???
        e_sum = e_sum * re_scale + gl.sum(p, 1)
        e_max = n_e_max
        # convert throught lds
        p = p.to(v_c.dtype)
        p = gl.convert_layout(p, mfma_layout_a)

        acc *= re_scale[:, None]
        v_c = buf_kv.load(layout=linear_v)
        v_c = gl.trans(v_c)
        v_c = gl.convert_layout(v_c, mfma_layout_b)
        acc += gl.dot(p.to(v_c.dtype), v_c)

    cur_head_o = cur_head_id * BLOCK_H + gl.arange(0, BLOCK_H, layout=gl.SliceLayout(1, mfma_layout))
    offs_d_ckv_o = gl.arange(0, HEAD_DIM_CKV, layout=gl.SliceLayout(0, mfma_layout))
    offs_o = cur_batch * stride_o_b + cur_head_o[:, None] * stride_o_h + split_kv_id * stride_o_s + offs_d_ckv_o[None, :]
    # gl.store(O + offs_o, acc / e_sum[:, None])
    gl.amd.cdna4.buffer_store(stored_value=acc / e_sum[:, None], ptr=O, offsets=offs_o, mask=None)
    offs_o_1 = cur_batch * stride_o_b + cur_head_o * stride_o_h + split_kv_id * stride_o_s + HEAD_DIM_CKV
    # gl.store(O + offs_o_1, e_max + gl.log(e_sum))
    gl.amd.cdna4.buffer_store(stored_value=e_max + gl.log(e_sum), ptr=O, offsets=offs_o_1, mask=None)

def _mla_attn(
    q_nope,
    q_pe,
    kv_c_cache,
    k_pe_cache,
    attn_logits,
    req_to_tokens,
    b_seq_len,
    num_kv_splits,
    sm_scale,
    page_size,
):
    batch_size, head_num = q_nope.shape[0], q_nope.shape[1]
    head_dim_ckv = q_nope.shape[-1]
    head_dim_kpe = q_pe.shape[-1]

    # BLOCK_H = 16
    # BLOCK_N = 64
    BLOCK_H = 64
    BLOCK_N = 16
    grid = (
        triton.cdiv(head_num, BLOCK_H),
        batch_size,
        num_kv_splits,
    )
    # _mla_attn_kernel[grid](
    _mla_attn_kernel_gluon[grid](
        q_nope,
        q_pe,
        kv_c_cache,
        k_pe_cache,
        req_to_tokens,
        b_seq_len,
        attn_logits,
        sm_scale,
        # stride
        q_nope.stride(0),
        q_nope.stride(1),
        q_pe.stride(0),
        q_pe.stride(1),
        kv_c_cache.stride(-2),
        k_pe_cache.stride(-2),
        req_to_tokens.stride(0),
        attn_logits.stride(0),
        attn_logits.stride(1),
        attn_logits.stride(2),
        BLOCK_H=BLOCK_H,
        BLOCK_N=BLOCK_N,
        NUM_KV_SPLITS=num_kv_splits,
        PAGE_SIZE=page_size,
        HEAD_DIM_CKV=head_dim_ckv,
        HEAD_DIM_KPE=head_dim_kpe,
    )

